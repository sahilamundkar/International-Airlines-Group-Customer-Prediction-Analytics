{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.810\n",
      "Test accuracy: 0.807\n"
     ]
    }
   ],
   "source": [
    "# pipeline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Define features\n",
    "categorical_features = [\n",
    "    'iag_business_unit_ug', 'iag_age_band_auto', 'iag_tenure_band_enum',\n",
    "    'iag_site_ug', 'iag_product_type_auto', 'iag_region_ug'\n",
    "]\n",
    "numeric_features = [\n",
    "    'iag_trust_confidence_scale11', 'iag_value_price_of_policy_reflects_scale11'\n",
    "]\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_excel('IAG.xlsx')\n",
    "X = df[categorical_features + numeric_features]\n",
    "y = df['Likely to recommend']\n",
    "\n",
    "# Remove detract classes and convert to binary (fixed capitalization)\n",
    "mask = ~y.isin(['Detract', 'Super Detract'])\n",
    "X = X[mask]\n",
    "y = (y[mask] == 'Promote').astype(int)  # 1 for Promote, 0 for Passive\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('binning', KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='quantile'))\n",
    "    ]), numeric_features),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), categorical_features)\n",
    "])\n",
    "\n",
    "# Create full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000, C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the full pipeline\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print training accuracy\n",
    "train_accuracy = full_pipeline.score(X_train, y_train)\n",
    "test_accuracy = full_pipeline.score(X_test, y_test)\n",
    "print(f'Training accuracy: {train_accuracy:.3f}')\n",
    "print(f'Test accuracy: {test_accuracy:.3f}')\n",
    "\n",
    "# Save pipeline\n",
    "with open('iag_full_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(full_pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key principle is: ANY transformation that learns from data (means, medians, modes, standard deviations, categories, bins, etc.) must only learn from training data and then apply that learned transformation to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain this with a clear example of what data leakage through scaling looks like:\n",
    "\n",
    "Let's say you have a dataset with a column for \"salary\" with these values:\n",
    "```python\n",
    "All data: [30000, 45000, 1000000, 55000, 48000]\n",
    "```\n",
    "\n",
    "When scaling (like StandardScaler), we typically:\n",
    "1. Calculate the mean (μ)\n",
    "2. Calculate the standard deviation (σ)\n",
    "3. Transform each value using: (x - μ) / σ\n",
    "\n",
    "Here's the crucial difference:\n",
    "\n",
    "BAD (Data Leakage):\n",
    "```python\n",
    "# Wrong way - using all data to calculate statistics\n",
    "all_data = [30000, 45000, 1000000, 55000, 48000]\n",
    "mean = np.mean(all_data)    # Influenced by test data!\n",
    "std = np.std(all_data)      # Influenced by test data!\n",
    "\n",
    "# Then splitting into train/test\n",
    "train_data = [30000, 45000, 1000000]\n",
    "test_data = [55000, 48000]\n",
    "\n",
    "# Scaling using statistics from ALL data\n",
    "scaled_train = [(x - mean) / std for x in train_data]\n",
    "scaled_test = [(x - mean) / std for x in test_data]\n",
    "```\n",
    "\n",
    "GOOD (No Leakage):\n",
    "```python\n",
    "# First split the data\n",
    "train_data = [30000, 45000, 1000000]\n",
    "test_data = [55000, 48000]\n",
    "\n",
    "# Calculate statistics ONLY from training data\n",
    "train_mean = np.mean(train_data)    # No test data influence\n",
    "train_std = np.std(train_data)      # No test data influence\n",
    "\n",
    "# Scale using ONLY training statistics\n",
    "scaled_train = [(x - train_mean) / train_std for x in train_data]\n",
    "scaled_test = [(x - train_mean) / train_std for x in test_data]\n",
    "```\n",
    "\n",
    "The key difference:\n",
    "- In the BAD example, we're \"cheating\" by letting our scaling know about the test data\n",
    "- In the GOOD example, we're being realistic - in the real world, we won't know the statistics of future (test) data\n",
    "\n",
    "This matters because:\n",
    "1. In the BAD example, our model gets an unrealistic advantage by \"seeing\" the distribution of ALL data\n",
    "2. The scaled values in our test set are influenced by their own values, which wouldn't be possible in a real-world scenario\n",
    "3. This can lead to overly optimistic model performance metrics that won't hold up in production\n",
    "\n",
    "This is why we use sklearn's Pipeline - it automatically handles this by fitting scalers only on training data:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Pipeline automatically only uses training data for scaling\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.predict(X_test)  # Test data scaled using training statistics\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
